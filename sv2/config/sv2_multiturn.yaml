# sv2 standalone config for multi-turn rollouts with sglang
# Independent of main verl trainer configs
# Configured for: Qwen3-0.6B (bf16) on H100

hydra:
  searchpath:
    # Path relative to this config file (sv2/config/)
    # ../../verl/trainer/config resolves to verl/trainer/config
    - file://../../verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

# Training mode flag
# When false, runs in eval mode (no weight updates, just rollouts + reward computation)
# When true, would enable PPO training (weight updates)
train: false

# Model configuration - Qwen3-0.6B on H100
actor_rollout_ref:
  hybrid_engine: true
  model:
    path: Qwen/Qwen3-0.6B
    use_shm: false
    dtype: bf16  # bf16 for H100

  actor:
    ppo_mini_batch_size: 32  # H100 can handle larger batches
    ppo_micro_batch_size_per_gpu: 32
    # Training params (used when train=true)
    optim:
      lr: 1e-6
      lr_warmup_steps_ratio: 0.0  # Fraction of total steps
      min_lr_ratio: 0.1  # END LR = start LR * min_lr_ratio
      weight_decay: 0.01

  rollout:
    name: sglang
    mode: async
    gpu_memory_utilization: 0.85
    tensor_model_parallel_size: 1
    data_parallel_size: 1
    pipeline_model_parallel_size: 1
    log_prob_micro_batch_size_per_gpu: 32
    free_cache_engine: false

    multi_turn:
      enable: true
      max_assistant_turns: 5
      max_user_turns: 3

    agent:
      num_workers: 1

# Critic configuration
critic:
  optim:
    lr: 1e-5
    lr_warmup_steps_ratio: 0.0
    min_lr_ratio: 0.1
    weight_decay: 0.01
  model:
    path: Qwen/Qwen3-0.6B # Use same as actor or reward model
    use_shm: false
    dtype: bf16
  ppo_mini_batch_size: 32
  ppo_micro_batch_size_per_gpu: 32

# Algorithm configuration
algorithm:
  gamma: 0.99
  lam: 0.95
  adv_estimator: gae
  kl_penalty: kl
  kl_ctrl:
    type: fixed
    kl_coef: 0.001

# Data configuration
data:
  max_prompt_length: 2048  # Qwen3 supports longer context
  max_response_length: 2048
  train_batch_size: 64  # H100 can handle larger batches
  return_raw_chat: true  # Required for AgentLoop
  trust_remote_code: false
  # train_files and val_files set via CLI
  dataloader_num_workers: 4

# sv2-specific configuration
sv2:
  split: val
  batch_size: 64  # Larger batch for H100
  max_batches: 1
  max_samples: -1
  dump_jsonl: null  # Set to path to dump rollout results
  interaction_name: null  # Set to interaction name (e.g., "code_verify")

# Training configuration (used when train=true)
# Training is a PLACEHOLDER - actual PPO update not implemented yet
training:
  total_steps: 100  # Total training steps
  eval_every_n_steps: 10  # Run eval every N steps
  save_every_n_steps: 50  # Save checkpoint every N steps (placeholder)
  # PPO hyperparams (placeholders for when training is implemented)
  ppo_epochs: 1
  learning_rate: 1e-6
  clip_ratio: 0.2
  gamma: 0.99
  gae_lambda: 0.95

# Trainer (minimal when train=false)
trainer:
  nnodes: 1
  n_gpus_per_node: 1
  project_name: sv2
  experiment_name: multiturn_rollout
  logger: ["console"]

# Ray configuration
ray_kwargs:
  ray_init:
    num_cpus: 8  # More CPUs for H100 system
